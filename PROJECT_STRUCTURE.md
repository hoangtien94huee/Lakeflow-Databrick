# ğŸ“ Lakeflow Demo - DLT + dbt Hybrid Architecture

## Project Structure

```
lakeflow-demo/
â”œâ”€â”€ ğŸ“„ databricks.yml                    # Databricks Asset Bundle config
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ README.md
â”‚
â”œâ”€â”€ ğŸ“‚ resources/                        # Databricks resources
â”‚   â”œâ”€â”€ pipeline.yml                     # DLT pipelines (ingestion only)
â”‚   â”œâ”€â”€ dbt_job.yml                      # dbt transformation job
â”‚   â””â”€â”€ workflow.yml                     # Combined workflows
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ sql/                          # DLT SQL (ingestion only)
â”‚   â”‚   â””â”€â”€ bronze.sql                   # Streaming ingestion
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ python/                       # DLT Python notebooks (ingestion)
â”‚       â””â”€â”€ bronze_to_silver.ipynb       # Alternative ingestion
â”‚
â”œâ”€â”€ ğŸ“‚ dbt_project/                      # â† NEW: dbt project
â”‚   â”œâ”€â”€ dbt_project.yml                  # dbt project config
â”‚   â”œâ”€â”€ profiles.yml                     # Databricks connection
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ staging/                  # Silver layer
â”‚   â”‚   â”‚   â”œâ”€â”€ schema.yml               # Tests & docs
â”‚   â”‚   â”‚   â””â”€â”€ stg_sales.sql            # Clean bronze â†’ silver
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ marts/                    # Gold layer
â”‚   â”‚   â”‚   â”œâ”€â”€ schema.yml
â”‚   â”‚   â”‚   â””â”€â”€ gold_sales_by_category.sql
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“‚ reports/                  # Report layer
â”‚   â”‚       â”œâ”€â”€ schema.yml
â”‚   â”‚       â”œâ”€â”€ rpt_executive_summary.sql
â”‚   â”‚       â”œâ”€â”€ rpt_category_ranking.sql
â”‚   â”‚       â””â”€â”€ rpt_gold_report.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ tests/                        # Data quality tests
â”‚   â”‚   â””â”€â”€ assert_positive_sales.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ macros/                       # Reusable SQL functions
â”‚   â”‚   â””â”€â”€ generate_schema_name.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ snapshots/                    # Slowly changing dimensions
â”‚   â”‚   â””â”€â”€ snapshot_categories.sql
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ seeds/                        # Static reference data
â”‚       â””â”€â”€ category_mapping.csv
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                            # Integration tests
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â””â”€â”€ ğŸ“‚ docs/                             # Documentation
    â”œâ”€â”€ ARCHITECTURE.md
    â””â”€â”€ DBT_GUIDE.md
```

## Data Flow

### Option 1: DLT Ingestion â†’ dbt Transformation (RECOMMENDED)

```
CSV â†’ DLT Bronze (streaming) â†’ dbt Silver â†’ dbt Gold â†’ dbt Reports â†’ Power BI
      ^^^^^^^^^^^^^^^^^^^^^^    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      Auto-loader, fast          Full testing, docs, version control
```

**Responsibilities:**
- **DLT**: Ingestion only (bronze layer with streaming)
- **dbt**: All transformations (silver, gold, reports)

### Option 2: Full dbt (Alternative)

```
CSV â†’ dbt Bronze â†’ dbt Silver â†’ dbt Gold â†’ dbt Reports â†’ Power BI
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      Pure dbt, simpler but no streaming
```

### Option 3: Hybrid with both DLT pipelines (Current + dbt)

```
Option A: CSV â†’ DLT SQL Pipeline â†’ Catalog Tables
Option B: CSV â†’ DLT Python Pipeline â†’ Catalog Tables
Option C: CSV â†’ dbt Pipeline â†’ Catalog Tables
```

Choose based on use case!

## Workflows

### Workflow 1: DLT Ingestion + dbt Transform
```yaml
1. Run DLT bronze pipeline (streaming ingestion)
2. Wait for completion
3. Run dbt (transformations)
4. dbt test (data quality)
5. dbt docs generate
```

### Workflow 2: Full dbt (Simpler)
```yaml
1. dbt seed (load reference data)
2. dbt run (all models)
3. dbt test (data quality)
4. dbt docs generate
```

## Next Steps

1. âœ… Setup dbt project structure
2. âœ… Configure dbt-databricks adapter
3. âœ… Migrate SQL transformations to dbt models
4. âœ… Add data quality tests
5. âœ… Setup dbt workflow in Databricks
6. âœ… Generate documentation
