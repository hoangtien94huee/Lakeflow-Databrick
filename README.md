# ğŸš€ Lakeflow Report Demo - Databricks Asset Bundles

**Demo project hoÃ n chá»‰nh** vá» Databricks Asset Bundles vá»›i Delta Live Tables Ä‘á»ƒ táº¡o pipeline xá»­ lÃ½ dá»¯ liá»‡u end-to-end: Ingestion â†’ Transformation â†’ Reporting â†’ Power BI.

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [Tá»•ng Quan](#-tá»•ng-quan)
2. [Quick Start - 5 PhÃºt](#-quick-start---5-phÃºt)
3. [CÃ i Äáº·t Chi Tiáº¿t](#-cÃ i-Ä‘áº·t-chi-tiáº¿t)
4. [Giáº£i Quyáº¿t DBFS Disabled](#-giáº£i-quyáº¿t-dbfs-disabled)
5. [Databricks CLI Commands](#-databricks-cli-commands)
6. [Architecture & Data Flow](#-architecture--data-flow)
7. [Test & Validation](#-test--validation)
8. [Power BI Integration](#-power-bi-integration)
9. [Troubleshooting](#-troubleshooting)
10. [Advanced Topics](#-advanced-topics)

---

## ğŸ“¦ Tá»•ng Quan

### Cáº¥u TrÃºc Project

```
lakeflow-demo/
â”œâ”€â”€ ğŸ“„ databricks.yml                  # Bundle configuration
â”œâ”€â”€ ğŸ“‚ resources/
â”‚   â””â”€â”€ pipeline.yml                   # 2 pipeline definitions
â”œâ”€â”€ ğŸ“‚ src/
â”‚   # SQL Files - Pure SQL Pipeline (RECOMMENDED)
â”‚   â”œâ”€â”€ bronze.sql                     # Ingest CSV â†’ Bronze
â”‚   â”œâ”€â”€ silver.sql                     # Clean & validate â†’ Silver
â”‚   â”œâ”€â”€ gold.sql                       # Aggregate â†’ Gold
â”‚   â”œâ”€â”€ gold_report_sql.sql            # Report tables
â”‚   # Python Notebooks - Alternative Pipeline
â”‚   â”œâ”€â”€ bronze_to_silver.ipynb
â”‚   â”œâ”€â”€ silver_to_gold.ipynb
â”‚   â””â”€â”€ gold_report_python.ipynb
â”œâ”€â”€ ğŸ generate_sample_data.py         # Test data generator
â”œâ”€â”€ ğŸ“Š test_queries.sql                # Validation SQL
â””â”€â”€ ğŸ“– README.md                       # This file
```

### Pipeline Versions

| Version | Files | Recommended For |
|---------|-------|-----------------|
| **SQL Pipeline** | `bronze.sql` â†’ `silver.sql` â†’ `gold.sql` â†’ `gold_report_sql.sql` | âœ… Production, pure SQL workflows |
| **Python Pipeline** | `*.ipynb` notebooks | Data science, visualizations |

### Data Flow

```
CSV File â†’ Bronze (Raw) â†’ Silver (Cleaned) â†’ Gold (Aggregated) â†’ Reports (Power BI Ready)
```

### Tables Created

| Layer | Table Name | Description |
|-------|------------|-------------|
| ğŸ¥‰ Bronze | `main.demo.bronze_table` | Raw CSV data |
| ğŸ¥ˆ Silver | `main.demo.silver_table` | Cleaned & validated |
| ğŸ¥‡ Gold | `main.demo.gold_table` | Aggregated by category |
| ğŸ“Š Report | `main.demo.gold_report_sql` | Power BI metrics |
| ğŸ“ˆ Report | `main.demo.executive_summary` | Executive KPIs |
| ğŸ† Report | `main.demo.category_ranking` | Performance ranks |

---

## âš¡ Quick Start - 5 PhÃºt

### BÆ°á»›c 1: CÃ i Databricks CLI (1 phÃºt)

```bash
# Install
pip install databricks-cli

# Verify
databricks -v
# Expected: Databricks CLI v0.270.0
```

### BÆ°á»›c 2: Authenticate (1 phÃºt)

```bash
# Method 1: Browser login (Recommended for v0.270.0+)
databricks auth login --host https://community.cloud.databricks.com

# Method 2: Token-based (Traditional)
databricks configure --token
# Host: https://community.cloud.databricks.com
# Token: <your-personal-access-token>

# Verify connection
databricks current-user me
```

**Táº¡o Personal Access Token:**
1. Login Databricks â†’ User Settings (gÃ³c pháº£i trÃªn)
2. Developer â†’ Access tokens â†’ Generate new token
3. Copy token (chá»‰ hiá»ƒn thá»‹ 1 láº§n!)

### BÆ°á»›c 3: Upload Dá»¯ Liá»‡u (1 phÃºt)

```bash
# Generate sample data
python generate_sample_data.py
```

**âš ï¸ QUAN TRá»ŒNG: Náº¿u gáº·p lá»—i "Public DBFS root is disabled"**

Sá»­ dá»¥ng Unity Catalog Volumes (xem [Section 4](#-giáº£i-quyáº¿t-dbfs-disabled)):

```sql
-- Trong Databricks SQL Editor
CREATE SCHEMA IF NOT EXISTS main.demo;
CREATE VOLUME IF NOT EXISTS main.demo.data_volume;
```

Sau Ä‘Ã³ upload qua UI: **Catalog â†’ main â†’ demo â†’ data_volume â†’ Upload files**

**Náº¿u DBFS enabled:**
```bash
databricks fs mkdirs dbfs:/FileStore/demo/
databricks fs cp sample_sales_data.csv dbfs:/FileStore/demo/input.csv
```

### BÆ°á»›c 4: Deploy Pipeline (1 phÃºt)

```bash
# Validate
databricks bundle validate

# Deploy
databricks bundle deploy -t dev
```

### BÆ°á»›c 5: Run Pipeline (1 phÃºt)

```bash
# Run SQL pipeline
databricks bundle run report_pipeline_sql --monitor

# Or Python pipeline
databricks bundle run report_pipeline --monitor
```

### BÆ°á»›c 6: Verify Results

```sql
-- Trong Databricks SQL Editor
SELECT * FROM main.demo.gold_table;
SELECT * FROM main.demo.gold_report_sql;
SELECT * FROM main.demo.executive_summary;
```

âœ… **Done! Pipeline Ä‘Ã£ cháº¡y thÃ nh cÃ´ng!**

---

## ğŸ”§ CÃ i Äáº·t Chi Tiáº¿t

### Prerequisites

- Python 3.8+
- Databricks workspace (Community Edition OK)
- Databricks CLI v0.270.0+
- Internet connection

### Step 1: Clone/Download Project

```bash
cd c:\inter-k\lakeflow-demo
# Or your project directory
```

### Step 2: Install Dependencies

```bash
# Install Databricks CLI
pip install databricks-cli

# Verify installation
databricks --version
```

### Step 3: Configure Authentication

#### Option A: Browser Login (New CLI v0.270.0+)

```bash
databricks auth login --host https://community.cloud.databricks.com
```

Browser sáº½ má»Ÿ vÃ  báº¡n login qua Databricks UI.

#### Option B: Token Configuration (Traditional)

```bash
databricks configure --token
```

Nháº­p:
- **Databricks Host**: `https://community.cloud.databricks.com`
- **Token**: `dapi_xxxxxxxxxxxx` (from User Settings)

#### Option C: Environment Variables

```powershell
# PowerShell
$env:DATABRICKS_HOST = "https://community.cloud.databricks.com"
$env:DATABRICKS_TOKEN = "dapi_your_token"
```

### Step 4: Generate Test Data

```bash
# Run Python script
python generate_sample_data.py
```

Output: `sample_sales_data.csv` vá»›i 100 transactions.

Sample data structure:
```csv
transaction_id,date,category,product_name,sales_amount,quantity,customer_id
TXN0001,2024-01-01,ELECTRONICS,Laptop,1200.00,1,CUST001
TXN0002,2024-01-02,CLOTHING,T-Shirt,25.99,2,CUST002
```

Categories: Electronics, Clothing, Books, Home & Garden, Sports

### Step 5: Upload Data

Chá»n 1 trong cÃ¡c options:

#### Option A: Unity Catalog Volumes (RECOMMENDED - works everywhere)

```sql
-- 1. Create schema and volume (Databricks SQL Editor)
CREATE SCHEMA IF NOT EXISTS main.demo;
CREATE VOLUME IF NOT EXISTS main.demo.data_volume;
```

```bash
-- 2. Upload via CLI
databricks fs cp sample_sales_data.csv /Volumes/main/demo/data_volume/input.csv

-- 3. Verify
databricks fs ls /Volumes/main/demo/data_volume/
```

Or upload via UI:
1. Databricks â†’ **Catalog**
2. Navigate: **main** â†’ **demo** â†’ **data_volume**
3. Click **Upload files**
4. Select `sample_sales_data.csv`

#### Option B: DBFS FileStore (if enabled)

```bash
databricks fs mkdirs dbfs:/FileStore/demo/
databricks fs cp sample_sales_data.csv dbfs:/FileStore/demo/input.csv
databricks fs ls dbfs:/FileStore/demo/
```

#### Option C: Create Table from CSV

1. Databricks UI â†’ **Data** â†’ **Create Table**
2. Upload `sample_sales_data.csv`
3. Table name: `main.demo.temp_sales_data`

Then modify `bronze.sql` to read from table instead of CSV.

### Step 6: Validate Bundle Configuration

```bash
# Navigate to project directory
cd c:\inter-k\lakeflow-demo

# Validate
databricks bundle validate
```

Expected output:
```
âœ“ Configuration valid
âœ“ Bundle name: lakeflow-report-demo
âœ“ Target: dev
```

### Step 7: Deploy Pipeline

```bash
# Deploy to dev environment
databricks bundle deploy -t dev
```

Expected output:
```
âœ“ Uploading bundle files...
âœ“ Deploying resources...
âœ“ Pipeline 'lakeflow-report-demo-pipeline-sql' created
âœ“ Deployment complete
```

### Step 8: Run Pipeline

```bash
# Run SQL pipeline with monitoring
databricks bundle run report_pipeline_sql --monitor
```

Pipeline stages:
1. â³ Bronze: Ingesting CSV â†’ bronze_table
2. â³ Silver: Cleaning & validating â†’ silver_table
3. â³ Gold: Aggregating â†’ gold_table
4. â³ Reports: Creating summary tables

### Step 9: Check Pipeline Status

```bash
# List all pipelines
databricks pipelines list

# Get pipeline details
databricks pipelines get lakeflow-report-demo-pipeline-sql

# View pipeline updates/logs
databricks pipelines list-updates lakeflow-report-demo-pipeline-sql
```

### Step 10: Verify Data

```sql
-- In Databricks SQL Editor

-- Check all tables
SHOW TABLES IN main.demo;

-- Count records per layer
SELECT 'Bronze' as layer, COUNT(*) as records FROM main.demo.bronze_table
UNION ALL
SELECT 'Silver', COUNT(*) FROM main.demo.silver_table
UNION ALL
SELECT 'Gold', COUNT(*) FROM main.demo.gold_table;

-- View gold table
SELECT * FROM main.demo.gold_table ORDER BY total_sales DESC;

-- View reports
SELECT * FROM main.demo.gold_report_sql;
SELECT * FROM main.demo.executive_summary;
SELECT * FROM main.demo.category_ranking;
```

---

## ğŸ›¡ï¸ Giáº£i Quyáº¿t DBFS Disabled

### Váº¥n Äá»

Náº¿u gáº·p lá»—i:
```
Error: Public DBFS root is disabled. Access is denied on path: /FileStore/demo
```

ÄÃ¢y lÃ  do Databricks Community Edition hoáº·c workspace cá»§a báº¡n Ä‘Ã£ disable DBFS public root vÃ¬ security reasons.

### Giáº£i PhÃ¡p: Unity Catalog Volumes

Unity Catalog Volumes lÃ  cÃ¡ch hiá»‡n Ä‘áº¡i vÃ  secure Ä‘á»ƒ lÆ°u files trong Databricks.

#### Solution 1: Táº¡o Volume qua UI (EASIEST)

**BÆ°á»›c 1: Táº¡o Volume**
1. VÃ o Databricks workspace
2. Click **"Catalog"** trÃªn left sidebar
3. Navigate: **main** â†’ **demo**
4. Click **"Create"** â†’ **"Volume"**
5. Volume Name: `data_volume`
6. Volume Type: `MANAGED`
7. Click **"Create"**

**BÆ°á»›c 2: Upload File**
1. Click vÃ o volume `data_volume`
2. Click **"Upload files"**
3. Select `sample_sales_data.csv`
4. Upload

**BÆ°á»›c 3: Verify**
```sql
SELECT * FROM csv.`/Volumes/main/demo/data_volume/input.csv` LIMIT 10;
```

âœ… **Done!** File `bronze.sql` Ä‘Ã£ Ä‘Æ°á»£c config sáºµn Ä‘á»ƒ Ä‘á»c tá»« Volumes.

#### Solution 2: Táº¡o Volume qua SQL

```sql
-- Create schema
CREATE SCHEMA IF NOT EXISTS main.demo;

-- Create volume
CREATE VOLUME IF NOT EXISTS main.demo.data_volume;

-- Verify
SHOW VOLUMES IN main.demo;

-- Describe volume
DESCRIBE VOLUME main.demo.data_volume;
```

Upload file qua CLI:
```bash
databricks fs cp sample_sales_data.csv /Volumes/main/demo/data_volume/input.csv
databricks fs ls /Volumes/main/demo/data_volume/
```

#### Solution 3: Táº¡o Volume qua Python Notebook

```python
# In Databricks notebook
from pyspark.sql import SparkSession

# Create schema
spark.sql("CREATE SCHEMA IF NOT EXISTS main.demo")

# Create volume
spark.sql("CREATE VOLUME IF NOT EXISTS main.demo.data_volume")

# Upload data
df = spark.read.csv(
    "file:/Workspace/Users/your-email/sample_sales_data.csv",
    header=True
)
df.write.mode("overwrite").csv(
    "/Volumes/main/demo/data_volume/input.csv",
    header=True
)

print("âœ… Data uploaded to Volume!")
```

#### Solution 4: Sá»­ dá»¥ng Workspace Files

Upload qua **Workspace â†’ Your folder â†’ Upload**

Path sáº½ lÃ : `/Workspace/Users/<your-email>/sample_sales_data.csv`

Modify `bronze.sql` Ä‘á»ƒ Ä‘á»c tá»« workspace path.

#### Solution 5: Direct Table Upload

1. **Data** â†’ **Create Table**
2. Upload CSV
3. Table: `main.demo.temp_sales_data`
4. Modify `bronze.sql` Ä‘á»ƒ Ä‘á»c tá»« table:

```sql
CREATE OR REFRESH LIVE TABLE bronze_table
AS SELECT
  transaction_id,
  date,
  category,
  product_name,
  sales_amount,
  quantity,
  customer_id,
  current_timestamp() AS ingestion_timestamp,
  'uploaded_table' AS source_file
FROM main.demo.temp_sales_data;
```

### Storage Options Comparison

| Method | Works with DBFS Disabled? | Security | Best For |
|--------|---------------------------|----------|----------|
| **Unity Catalog Volumes** | âœ… Yes | ğŸ”’ High | Production (RECOMMENDED) |
| DBFS FileStore | âŒ No | âš ï¸ Medium | Legacy only |
| Workspace Files | âœ… Yes | ğŸ”’ Medium | Development |
| Direct Tables | âœ… Yes | ğŸ”’ High | Simple use cases |

### Default Configuration

File `bronze.sql` Ä‘Ã£ Ä‘Æ°á»£c config Ä‘á»ƒ sá»­ dá»¥ng Unity Catalog Volumes:

```sql
-- Default: Unity Catalog Volumes
FROM cloud_files(
  '/Volumes/main/demo/data_volume/',
  'csv',
  map('header', 'true', ...)
);
```

Náº¿u cáº§n dÃ¹ng DBFS, uncomment option 2 trong file.

---

## ğŸ’» Databricks CLI Commands

### CLI Version

Project nÃ y há»— trá»£ **Databricks CLI v0.270.0+**

```bash
# Check version
databricks -v

# Update if needed
pip install --upgrade databricks-cli
```

### Command Changes in v0.270.0+

| Old CLI | New CLI v0.270.0+ |
|---------|-------------------|
| `databricks configure --token` | `databricks auth login --host <host>` |
| `databricks workspace ls /` | `databricks workspace list /` |
| `databricks pipelines get --pipeline-id <id>` | `databricks pipelines get <id>` |
| `databricks pipelines get-events --pipeline-id <id>` | `databricks pipelines list-updates <id>` |

### Essential Commands

#### Authentication
```bash
# Login (new method)
databricks auth login --host https://community.cloud.databricks.com

# Configure (traditional - still works)
databricks configure --token

# Check current user
databricks current-user me

# List profiles
databricks auth profiles
```

#### Bundle Commands
```bash
# Validate bundle
databricks bundle validate

# Deploy to dev
databricks bundle deploy -t dev

# Run pipeline
databricks bundle run report_pipeline_sql
databricks bundle run report_pipeline_sql --monitor

# Destroy bundle
databricks bundle destroy -t dev
```

#### Pipeline Commands
```bash
# List all pipelines
databricks pipelines list

# Get pipeline details
databricks pipelines get <pipeline-name-or-id>

# List pipeline updates (logs/events)
databricks pipelines list-updates <pipeline-id>

# Get specific update details
databricks pipelines get-update <pipeline-id> <update-id>

# Start pipeline
databricks pipelines start <pipeline-id>

# Stop pipeline
databricks pipelines stop <pipeline-id>

# Delete pipeline
databricks pipelines delete <pipeline-id>
```

#### File System (DBFS/Volumes)
```bash
# List files
databricks fs ls /Volumes/main/demo/data_volume/
databricks fs ls dbfs:/FileStore/demo/

# Copy file to DBFS/Volume
databricks fs cp local.csv /Volumes/main/demo/data_volume/file.csv
databricks fs cp local.csv dbfs:/path/

# Remove file
databricks fs rm /Volumes/main/demo/data_volume/old.csv

# Create directory
databricks fs mkdirs dbfs:/FileStore/demo/

# Read file content
databricks fs cat /Volumes/main/demo/data_volume/input.csv
databricks fs head dbfs:/FileStore/demo/input.csv
```

#### Workspace Commands
```bash
# List workspace items
databricks workspace list /
databricks workspace list /Users/

# Upload file
databricks workspace upload file.py /Users/me/file.py

# Download file
databricks workspace download /Users/me/file.py local-file.py

# Delete item
databricks workspace delete /Users/me/old-file.py
```

#### Cluster Commands
```bash
# List clusters
databricks clusters list

# Get cluster details
databricks clusters get <cluster-id>

# Start/Stop cluster
databricks clusters start <cluster-id>
databricks clusters stop <cluster-id>
```

### Environment Variables

```powershell
# PowerShell
$env:DATABRICKS_HOST = "https://community.cloud.databricks.com"
$env:DATABRICKS_TOKEN = "dapi_your_token_here"
$env:DATABRICKS_BUNDLE_TARGET = "dev"

# Then run commands (automatically uses env vars)
databricks bundle deploy
```

---

## ğŸ—ï¸ Architecture & Data Flow

### Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA SOURCE                              â”‚
â”‚  CSV: /Volumes/main/demo/data_volume/input.csv              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BRONZE LAYER (Raw)                          â”‚
â”‚  Table: main.demo.bronze_table                               â”‚
â”‚  - Cloud Files auto-loader                                   â”‚
â”‚  - Schema validation only                                    â”‚
â”‚  - All data ingested (no filtering)                          â”‚
â”‚  File: bronze.sql                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SILVER LAYER (Cleaned)                       â”‚
â”‚  Table: main.demo.silver_table                               â”‚
â”‚  âœ“ Data Quality Checks:                                      â”‚
â”‚    - sales_amount > 0                                        â”‚
â”‚    - quantity > 0                                            â”‚
â”‚    - date IS NOT NULL                                        â”‚
â”‚    - category IS NOT NULL                                    â”‚
â”‚  âœ“ Transformations:                                          â”‚
â”‚    - Date conversion, UPPER(category), TRIM()                â”‚
â”‚    - Calculate: total_amount = sales Ã— quantity              â”‚
â”‚  âœ“ Invalid rows â†’ DROPPED                                    â”‚
â”‚  File: silver.sql                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GOLD LAYER (Aggregated)                     â”‚
â”‚  Table: main.demo.gold_table                                 â”‚
â”‚  ğŸ“Š Aggregations by Category:                                â”‚
â”‚    - total_transactions, total_sales                         â”‚
â”‚    - avg_transaction_value                                   â”‚
â”‚    - total_quantity_sold                                     â”‚
â”‚    - unique_customers (COUNT DISTINCT)                       â”‚
â”‚    - first/last_transaction_date                             â”‚
â”‚  File: gold.sql                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               REPORTING LAYER (Analytics)                    â”‚
â”‚  ğŸ“ˆ gold_report_sql - Detailed KPIs per category             â”‚
â”‚  ğŸ“Š executive_summary - Overall business metrics             â”‚
â”‚  ğŸ† category_ranking - Performance rankings                  â”‚
â”‚  File: gold_report_sql.sql                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚           â”‚                    â”‚
             â–¼           â–¼                    â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚Power BI  â”‚ â”‚ Tableau  â”‚      â”‚  Python  â”‚
      â”‚Dashboard â”‚ â”‚ Reports  â”‚      â”‚ Notebooksâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Quality Framework

| Layer | Quality Level | Validations | Invalid Rows |
|-------|---------------|-------------|--------------|
| Bronze | Low | Schema only | All kept |
| Silver | Medium | 5 constraints | Dropped |
| Gold | High | Business logic | N/A (aggregated) |
| Report | Very High | KPI calculations | N/A |

### Technology Stack

- **Delta Live Tables (DLT)**: Declarative ETL pipeline
- **Delta Lake**: ACID transactions, time travel, schema evolution
- **Unity Catalog**: Data governance, permissions, lineage
- **Databricks Asset Bundles**: Infrastructure as Code, CI/CD
- **Cloud Files**: Auto file discovery, incremental processing

---

## âœ… Test & Validation

### Run Test Queries

File `test_queries.sql` chá»©a comprehensive test queries.

```sql
-- Bronze layer tests
SELECT COUNT(*) as total_records FROM main.demo.bronze_table;

SELECT category, COUNT(*) as count 
FROM main.demo.bronze_table 
GROUP BY category;

-- Silver layer tests
SELECT 
    'Bronze' as layer,
    COUNT(*) as records
FROM main.demo.bronze_table
UNION ALL
SELECT 
    'Silver',
    COUNT(*)
FROM main.demo.silver_table;

-- Gold layer tests
SELECT * FROM main.demo.gold_table
ORDER BY total_sales DESC;

-- Report tests
SELECT * FROM main.demo.gold_report_sql;
SELECT * FROM main.demo.executive_summary;
SELECT * FROM main.demo.category_ranking;

-- Data quality checks
SELECT 
    COUNT(*) as total_rows,
    SUM(CASE WHEN category IS NULL THEN 1 ELSE 0 END) as null_categories,
    SUM(CASE WHEN total_sales IS NULL THEN 1 ELSE 0 END) as null_sales
FROM main.demo.gold_table;

-- Verify calculations
SELECT 
    category,
    total_sales,
    total_transactions,
    avg_transaction_value,
    ROUND(total_sales / total_transactions, 2) as calculated_avg,
    CASE 
        WHEN ABS(avg_transaction_value - (total_sales / total_transactions)) < 0.01 
        THEN 'PASS' 
        ELSE 'FAIL' 
    END as validation_status
FROM main.demo.gold_table;
```

### Expected Results

**Bronze Table:**
- All 100 records from CSV
- Includes invalid/dirty data
- Has metadata columns (ingestion_timestamp, source_file)

**Silver Table:**
- â‰¤ 100 records (invalid rows dropped)
- All data quality constraints passed
- Cleaned and standardized data

**Gold Table:**
- 5 rows (one per category)
- Aggregated metrics per category
- Sorted by total_sales DESC

**Report Tables:**
- `gold_report_sql`: Detailed KPIs with percentages
- `executive_summary`: Single row with overall metrics
- `category_ranking`: Ranked categories with ranks

---

## ğŸ“Š Power BI Integration

### Prerequisites

1. Databricks SQL Warehouse (create in Databricks UI)
2. Server Hostname & HTTP Path (from SQL Warehouse)
3. Power BI Desktop installed

### Setup Steps

#### Step 1: Create SQL Warehouse

1. Databricks UI â†’ **SQL Warehouses**
2. Click **"Create SQL Warehouse"**
3. Name: `reporting-warehouse`
4. Cluster size: `X-Small` (for dev)
5. **Create**

#### Step 2: Get Connection Details

1. Click on SQL Warehouse
2. **Connection Details** tab
3. Copy:
   - **Server hostname**: `your-workspace.cloud.databricks.com`
   - **HTTP path**: `/sql/1.0/warehouses/xxxxx`

#### Step 3: Connect from Power BI

1. Open **Power BI Desktop**
2. **Get Data** â†’ **More** â†’ Search "Databricks"
3. Select **"Azure Databricks"**
4. Enter:
   - **Server hostname**: (from step 2)
   - **HTTP path**: (from step 2)
5. **OK**
6. Authentication:
   - **Personal Access Token**
   - Enter your Databricks token
7. **Connect**

#### Step 4: Select Tables

Navigator window will show:
1. Expand **main** â†’ **demo**
2. Select tables:
   - âœ… `gold_report_sql`
   - âœ… `executive_summary`
   - âœ… `category_ranking`
   - âœ… `gold_table` (optional)
3. **Load**

#### Step 5: Create Visualizations

**Recommended Visuals:**

| Visual Type | Data Source | Fields |
|-------------|-------------|--------|
| **KPI Card** | executive_summary | total_revenue, total_transactions_count |
| **Bar Chart** | gold_report_sql | category (axis), total_sales (values) |
| **Pie Chart** | gold_report_sql | category (legend), sales_percentage (values) |
| **Table** | category_ranking | All fields |
| **Line Chart** | gold_table | category (legend), total_sales (values) |
| **Gauge** | gold_report_sql | avg_transaction_value |

**Sample Dashboard Layout:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Total Revenue      Total Transactions    Avg Value â”‚
â”‚  [$XX,XXX]         [XXX]                 [$XX.XX]   â”‚ â† KPI Cards
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚    Sales by Category (Bar Chart)                     â”‚ â† Bar Chart
â”‚    â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚               â”‚
â”‚                                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Revenue Distribution    â”‚   Category Rankings      â”‚
â”‚  (Pie Chart)             â”‚   (Table)                â”‚ â† Pie + Table
â”‚       ğŸ“Š                  â”‚   Rank  Category  Sales  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Power BI Query Examples

```powerquery
// Filter high performers
= Table.SelectRows(gold_report_sql, each [performance_category] = "High Performer")

// Calculate custom metrics
= Table.AddColumn(gold_report_sql, "Profit Margin", each [total_sales] * 0.3)

// Sort by sales
= Table.Sort(gold_report_sql,{{"total_sales", Order.Descending}})
```

### Refresh Data

**Manual Refresh:**
- Power BI â†’ **Home** â†’ **Refresh**

**Scheduled Refresh:**
1. Publish to Power BI Service
2. **Settings** â†’ **Scheduled refresh**
3. Configure refresh schedule (e.g., daily at 6 AM)

---

## ğŸ› Troubleshooting

### Issue 1: Public DBFS Root Disabled

**Error:**
```
Error: Public DBFS root is disabled. Access is denied on path: /FileStore/demo
```

**Solution:**
Use Unity Catalog Volumes (see [Section 4](#-giáº£i-quyáº¿t-dbfs-disabled))

Quick fix:
```sql
CREATE SCHEMA IF NOT EXISTS main.demo;
CREATE VOLUME IF NOT EXISTS main.demo.data_volume;
```

Upload via UI: Catalog â†’ main â†’ demo â†’ data_volume â†’ Upload

### Issue 2: File Not Found

**Error:**
```
FileNotFoundException: /Volumes/main/demo/data_volume/input.csv
```

**Solution:**
```bash
# Verify volume exists
databricks fs ls /Volumes/main/demo/data_volume/

# Re-upload if missing
databricks fs cp sample_sales_data.csv /Volumes/main/demo/data_volume/input.csv
```

### Issue 3: Permission Denied

**Error:**
```
Error: You don't have permission to access this resource
```

**Solution:**
1. Verify correct token in auth
2. Check workspace permissions
3. For Unity Catalog: Check schema/volume permissions
4. Re-authenticate:
   ```bash
   databricks auth login --host https://community.cloud.databricks.com
   ```

### Issue 4: Pipeline Validation Errors

**Error:**
```
Validation failed: Invalid table reference
```

**Solution:**
- Verify catalog: `main`
- Verify schema: `demo`
- Check SQL files use `LIVE.` prefix
- Ensure volume exists:
  ```sql
  SHOW VOLUMES IN main.demo;
  ```

### Issue 5: Empty Tables

**Error:**
Tables created but have 0 rows

**Solution:**
```bash
# Check pipeline logs
databricks pipelines list-updates lakeflow-report-demo-pipeline-sql

# Verify source data
databricks fs ls /Volumes/main/demo/data_volume/

# Check bronze table
# In SQL Editor: SELECT COUNT(*) FROM main.demo.bronze_table

# Re-run pipeline
databricks bundle run report_pipeline_sql
```

### Issue 6: Bundle Deployment Fails

**Error:**
```
Error: Failed to deploy bundle
```

**Solution:**
```bash
# Clean up previous deployment
databricks bundle destroy -t dev

# Validate first
databricks bundle validate

# Re-deploy
databricks bundle deploy -t dev
```

### Issue 7: CLI Version Issues

**Error:**
```
Error: unknown command "auth" for "databricks"
```

**Solution:**
Old CLI version. Update:
```bash
pip install --upgrade databricks-cli
databricks -v
```

Expected: `Databricks CLI v0.270.0` or higher

### Issue 8: Schema/Catalog Not Found

**Error:**
```
Schema not found: main.demo
```

**Solution:**
```sql
-- Create schema first
CREATE SCHEMA IF NOT EXISTS main.demo;

-- Verify
SHOW SCHEMAS IN main;

-- Create volume
CREATE VOLUME IF NOT EXISTS main.demo.data_volume;
```

### Common Error Patterns

| Error Message | Most Likely Cause | Quick Fix |
|---------------|-------------------|-----------|
| "DBFS root disabled" | DBFS security restriction | Use Unity Catalog Volumes |
| "File not found" | Data not uploaded | Upload CSV to volume |
| "Permission denied" | Invalid token/permissions | Re-authenticate |
| "Schema not found" | Schema doesn't exist | CREATE SCHEMA main.demo |
| "Pipeline failed" | Data quality issue | Check bronze table data |

---

## ğŸš€ Advanced Topics

### Scheduling Pipeline

Add to `resources/pipeline.yml`:

```yaml
configuration:
  "pipelines.trigger.interval": "cron"
  "pipelines.cron.expression": "0 0 * * *"  # Daily at midnight
```

### Email Notifications

```yaml
notifications:
  - email_recipients:
      - your-email@example.com
    alerts:
      - on-update-failure
      - on-update-success
```

### Multi-Environment Deployment

```bash
# Deploy to staging
databricks bundle deploy -t staging

# Deploy to production
databricks bundle deploy -t prod
```

Add to `databricks.yml`:
```yaml
targets:
  dev:
    mode: development
  staging:
    mode: development
    workspace:
      host: https://staging-workspace.databricks.com
  prod:
    mode: production
    workspace:
      host: https://prod-workspace.databricks.com
```

### Performance Optimization

**Z-Ordering:**
```sql
-- Already implemented in bronze.sql
TBLPROPERTIES ("pipelines.autoOptimize.zOrderCols" = "transaction_id")
```

**Partitioning (for large datasets):**
```sql
PARTITION BY (transaction_date)
```

**Auto Optimize:**
```yaml
configuration:
  "spark.databricks.delta.autoOptimize.optimizeWrite": "true"
  "spark.databricks.delta.autoOptimize.autoCompact": "true"
```

### Adding More Data Sources

**From S3:**
```sql
FROM cloud_files(
  's3://your-bucket/path/',
  'csv',
  map('header', 'true')
);
```

**From Delta Table:**
```sql
FROM delta.`/path/to/delta/table`
```

**From JDBC:**
```sql
FROM jdbc(
  'jdbc:mysql://host:port/database',
  'table_name',
  map('user', 'username', 'password', 'password')
)
```

### Data Lineage

View in Databricks UI:
1. **Data Explorer** â†’ **main.demo.gold_table**
2. **Lineage** tab
3. See upstream/downstream dependencies

### Custom Transformations

Add to `silver.sql`:
```sql
CASE 
  WHEN category = 'ELECTRONICS' THEN 'Tech'
  WHEN category = 'CLOTHING' THEN 'Fashion'
  ELSE 'Other'
END as category_group
```

### Testing in Notebooks

Create notebook:
```python
# Test bronze table
df_bronze = spark.table("main.demo.bronze_table")
print(f"Bronze records: {df_bronze.count()}")

# Test silver table
df_silver = spark.table("main.demo.silver_table")
print(f"Silver records: {df_silver.count()}")

# Verify data quality
df_silver.filter("sales_amount <= 0").count()  # Should be 0
```

---

## ğŸ“š Resources

### Databricks Documentation
- [Delta Live Tables](https://docs.databricks.com/delta-live-tables/)
- [Asset Bundles](https://docs.databricks.com/dev-tools/bundles/)
- [Databricks CLI](https://docs.databricks.com/dev-tools/cli/)
- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/)

### Community
- [Databricks Community](https://community.databricks.com/)
- [Stack Overflow - Databricks](https://stackoverflow.com/questions/tagged/databricks)

### Sample Data Schema

```
transaction_id: String       # TXN0001, TXN0002, ...
date: String                 # 2024-01-01, 2024-01-02, ...
category: String             # Electronics, Clothing, Books, Home & Garden, Sports
product_name: String         # Laptop, T-Shirt, Python Guide, ...
sales_amount: Double         # 1200.00, 25.99, 45.00, ...
quantity: Integer            # 1, 2, 3, ...
customer_id: String          # CUST001, CUST002, ...
```

---

## ğŸ‰ Summary

### What You Get

âœ… **Complete ETL Pipeline**: Bronze â†’ Silver â†’ Gold â†’ Reports  
âœ… **2 Pipeline Versions**: SQL (production) + Python (development)  
âœ… **Data Quality**: 5 validation rules in Silver layer  
âœ… **6 Tables Created**: bronze, silver, gold, + 3 report tables  
âœ… **Power BI Ready**: Pre-calculated KPIs and rankings  
âœ… **DBFS Workaround**: Unity Catalog Volumes solution  
âœ… **CLI v0.270.0+**: Updated for latest Databricks CLI  
âœ… **Sample Data**: 100 transactions across 5 categories  
âœ… **Test Queries**: Comprehensive validation SQL  
âœ… **Production Ready**: Error handling, monitoring, documentation  

### Next Steps

1. âœ… **Run the Quick Start** (5 minutes)
2. âœ… **Verify all tables** created successfully
3. âœ… **Connect Power BI** and build dashboards
4. âœ… **Schedule pipeline** for automatic runs
5. âœ… **Customize** for your data and business logic

---

## ğŸ“ Support

**Need Help?**

1. Check **[Troubleshooting](#-troubleshooting)** section
2. Review pipeline logs: `databricks pipelines list-updates <pipeline-id>`
3. Verify data: Run queries from `test_queries.sql`
4. Check Databricks documentation

**Common Issues:**
- **DBFS disabled** â†’ Use Unity Catalog Volumes (Section 4)
- **File not found** â†’ Verify upload (Section 3)
- **Pipeline failed** â†’ Check logs (Section 9)
- **Empty tables** â†’ Verify source data

---

**ğŸ¯ Ready to Deploy!**

Your complete Lakeflow pipeline is ready to run on Databricks! ğŸš€

*Last Updated: October 2025 | Compatible with Databricks CLI v0.270.0+*# Lakeflow-Databrick
